## 2023-8-17
* AI
  * Squared error cost function for linear regression model

***
## 2023-8-19
* AI
  * Gradient descent algorithm should simultaneously update w and b
  * convex function 凸函数 bowl-shape
  * "batch" gradient descent: each step of gradient descent uses all the training samples.

***
## 2023-8-19
* AI
  * multiple linear regression(linear regression with multiple features)
  * f = np.dot(w, x) + b

***
## 2023-8-23
* AI
  * if the different features take on different ranges of values, it causes gradient descent to run slowly. Need to rescale the features so they take on comparable range of values. (feature scaling)
    * way 1: 300 < x < 2000, x = x / 2000
    * way 2: mean normalization, x = (x - u) / (2000 - 300), to get the normalized x. usually range is (-1, 1)
    * Z-score normalization
  * check gradient descent for convergence
    * learning curve, x-axis is #iterations, y-axis is cost function value. J shoule decrease after every iteration.
    * automatic convergence test
  * Feature engineering: Using intuition to design new features, by transforming or combining original features.

***
## 2023-8-27